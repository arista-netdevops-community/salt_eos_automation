{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"This is a example repo for using Saltstack on Arista EOS using cEOS & docker.","title":"Home"},{"location":"#this-is-a-example-repo-for-using-saltstack-on-arista-eos-using-ceos-docker","text":"","title":"This is a example repo for using Saltstack on Arista EOS using cEOS &amp; docker."},{"location":"basic_bgp/","text":"In this example we will render a configuration that will render configurations for all devices to create a full BGP configuration using the saltstack state execution module called state.sls. Make sure that all proxy minions are running. root@a00c23e5231c:/srv/salt# salt '*' test.ping base_lab_Leaf1: True base_lab_Spine2: True base_lab_Spine1: True base_lab_Leaf2: True Everything to make the state work is located within the /srv/salt/states directory. Since the file roots use that as a directory salt is able to serve those files to the minions. Salt state basic_bgp_render.sls {% set hostname = grains.get('host') %} render the output: file.managed: - name: /srv/salt/templates/intended/configs/{{ hostname }}.cfg - source: /srv/salt/templates/basic_bgp.j2 - template: jinja Taking a look at this the state this will take a device like base_lab_leaf1 and use the jinja template which is located within /srv/salt/templates/basic_bgp.j2 and will render this using Jinja and save the output within /srv/salt/templates/intended/configs/base_lab_Leaf1.cfg. Just a small snippet of the jinja2 template. {% set hostname = grains.get('host') -%} {% set template_location = '/srv/salt/templates/intended/structured_configs/' + hostname + '.yaml' -%} {% import_yaml template_location as device_info -%} {% if device_info['leaf_allowed_vlans'] is defined and device_info['leaf_allowed_vlans'] is not none -%} {% for vlan in device_info['leaf_allowed_vlans'] -%} vlan {{ vlan }} Checking to see if the template will render. root@a00c23e5231c:/srv/salt# salt 'base_lab_Leaf1' slsutil.renderer /srv/salt/templates/basic_bgp.j2 'jinja' base_lab_Leaf1: vlan 130 ! vlan 131 ! vlan 140 ! hostname base_lab_Leaf1 To explain how this works this salt command will taget base_lab_Leaf1 and use the slsutil module and rendere function just to make sure that this rendering will actually work. Now that we know this will render correctly lets render all files. Rendering configuration rendering the basic_bgp_render.sls state root@a00c23e5231c:/srv/salt# salt '*' state.sls basic_bgp_render base_lab_Spine1: ---------- ID: render the output Function: file.managed Name: /srv/salt/templates/intended/configs/base_lab_Spine1.cfg Result: True Comment: File /srv/salt/templates/intended/configs/base_lab_Spine1.cfg is in the correct state Started: 15:20:53.617619 Duration: 425.391 ms Changes: The config is currently located within the /srv/salt/templates/intended/configs/base_lab_Spine1.cfg ready to be pushed out to all the devices a long with spine2,leaf1 and leaf2. root@a00c23e5231c:/srv/salt# ls -l templates/intended/configs/ total 32 -rwxr-xr-x 1 saltdev saltdev 7792 Nov 10 12:06 base_lab_Leaf1.cfg -rwxr-xr-x 1 saltdev saltdev 7798 Nov 10 12:06 base_lab_Leaf2.cfg -rwxr-xr-x 1 saltdev saltdev 2104 Nov 10 12:06 base_lab_Spine1.cfg -rwxr-xr-x 1 saltdev saltdev 2104 Nov 10 12:06 base_lab_Spine2.cfg Using the pyeapi salt module to push config We will use the pyeapi salt state module to push each of the configs to the devices. Example of /srv/salt/states/push_config.sls {% set hostname = grains.get('host') %} pyeapi.config: module.run: - config_file: /srv/salt/templates/intended/configs/{{hostname}}.cfg This is rather simple. This state will simply push out the configuration for each device based off of its own hostname. root@a00c23e5231c:/srv/salt# salt '*' state.sls push_config base_lab_Spine2: ---------- ID: pyeapi.config Function: module.run Result: True Comment: Module function pyeapi.config executed Started: 15:28:36.625957 Duration: 2562.588 ms Changes: ---------- ret: --- +++ @@ -1,23 +1,26 @@ +! +transceiver qsfp default-mode 4x10G ! service routing protocols model multi-agent ! logging host 1.2.3.4 514 ! -hostname spine2 +hostname base_lab_Spine2 Truncated most of the push for length of each device. SSH or exec into either devices. base_lab_Leaf2#show ip bgp summary BGP summary information for VRF default Router identifier 192.168.255.6, local AS number 65102 Neighbor Status Codes: m - Under maintenance Neighbor V AS MsgRcvd MsgSent InQ OutQ Up/Down State PfxRcd PfxAcc 10.0.0.6 4 65000 7 7 0 0 00:01:32 Estab 4 4 10.1.0.6 4 65000 7 7 0 0 00:01:32 Estab 4 4","title":"basic bgp"},{"location":"basic_bgp/#make-sure-that-all-proxy-minions-are-running","text":"root@a00c23e5231c:/srv/salt# salt '*' test.ping base_lab_Leaf1: True base_lab_Spine2: True base_lab_Spine1: True base_lab_Leaf2: True Everything to make the state work is located within the /srv/salt/states directory. Since the file roots use that as a directory salt is able to serve those files to the minions.","title":"Make sure that all proxy minions are running."},{"location":"basic_bgp/#salt-state-basic_bgp_rendersls","text":"{% set hostname = grains.get('host') %} render the output: file.managed: - name: /srv/salt/templates/intended/configs/{{ hostname }}.cfg - source: /srv/salt/templates/basic_bgp.j2 - template: jinja Taking a look at this the state this will take a device like base_lab_leaf1 and use the jinja template which is located within /srv/salt/templates/basic_bgp.j2 and will render this using Jinja and save the output within /srv/salt/templates/intended/configs/base_lab_Leaf1.cfg. Just a small snippet of the jinja2 template. {% set hostname = grains.get('host') -%} {% set template_location = '/srv/salt/templates/intended/structured_configs/' + hostname + '.yaml' -%} {% import_yaml template_location as device_info -%} {% if device_info['leaf_allowed_vlans'] is defined and device_info['leaf_allowed_vlans'] is not none -%} {% for vlan in device_info['leaf_allowed_vlans'] -%} vlan {{ vlan }}","title":"Salt state basic_bgp_render.sls"},{"location":"basic_bgp/#checking-to-see-if-the-template-will-render","text":"root@a00c23e5231c:/srv/salt# salt 'base_lab_Leaf1' slsutil.renderer /srv/salt/templates/basic_bgp.j2 'jinja' base_lab_Leaf1: vlan 130 ! vlan 131 ! vlan 140 ! hostname base_lab_Leaf1 To explain how this works this salt command will taget base_lab_Leaf1 and use the slsutil module and rendere function just to make sure that this rendering will actually work. Now that we know this will render correctly lets render all files.","title":"Checking to see if the template will render."},{"location":"basic_bgp/#rendering-configuration","text":"","title":"Rendering configuration"},{"location":"basic_bgp/#rendering-the-basic_bgp_rendersls-state","text":"root@a00c23e5231c:/srv/salt# salt '*' state.sls basic_bgp_render base_lab_Spine1: ---------- ID: render the output Function: file.managed Name: /srv/salt/templates/intended/configs/base_lab_Spine1.cfg Result: True Comment: File /srv/salt/templates/intended/configs/base_lab_Spine1.cfg is in the correct state Started: 15:20:53.617619 Duration: 425.391 ms Changes: The config is currently located within the /srv/salt/templates/intended/configs/base_lab_Spine1.cfg ready to be pushed out to all the devices a long with spine2,leaf1 and leaf2. root@a00c23e5231c:/srv/salt# ls -l templates/intended/configs/ total 32 -rwxr-xr-x 1 saltdev saltdev 7792 Nov 10 12:06 base_lab_Leaf1.cfg -rwxr-xr-x 1 saltdev saltdev 7798 Nov 10 12:06 base_lab_Leaf2.cfg -rwxr-xr-x 1 saltdev saltdev 2104 Nov 10 12:06 base_lab_Spine1.cfg -rwxr-xr-x 1 saltdev saltdev 2104 Nov 10 12:06 base_lab_Spine2.cfg","title":"rendering the basic_bgp_render.sls state"},{"location":"basic_bgp/#using-the-pyeapi-salt-module-to-push-config","text":"We will use the pyeapi salt state module to push each of the configs to the devices. Example of /srv/salt/states/push_config.sls {% set hostname = grains.get('host') %} pyeapi.config: module.run: - config_file: /srv/salt/templates/intended/configs/{{hostname}}.cfg This is rather simple. This state will simply push out the configuration for each device based off of its own hostname. root@a00c23e5231c:/srv/salt# salt '*' state.sls push_config base_lab_Spine2: ---------- ID: pyeapi.config Function: module.run Result: True Comment: Module function pyeapi.config executed Started: 15:28:36.625957 Duration: 2562.588 ms Changes: ---------- ret: --- +++ @@ -1,23 +1,26 @@ +! +transceiver qsfp default-mode 4x10G ! service routing protocols model multi-agent ! logging host 1.2.3.4 514 ! -hostname spine2 +hostname base_lab_Spine2 Truncated most of the push for length of each device.","title":"Using the pyeapi salt module to push config"},{"location":"basic_bgp/#ssh-or-exec-into-either-devices","text":"base_lab_Leaf2#show ip bgp summary BGP summary information for VRF default Router identifier 192.168.255.6, local AS number 65102 Neighbor Status Codes: m - Under maintenance Neighbor V AS MsgRcvd MsgSent InQ OutQ Up/Down State PfxRcd PfxAcc 10.0.0.6 4 65000 7 7 0 0 00:01:32 Estab 4 4 10.1.0.6 4 65000 7 7 0 0 00:01:32 Estab 4 4","title":"SSH or exec into either devices."},{"location":"bus/","text":"Salt utilizes a ZeroMQ bus within the communication between the salt master and salt minions. The salt master fires off events directly from the bus to the minions where the minion will receive structured data and that data is then passed into a module. Stop the master and put it in debug mode. service salt-master stop Debug the salt master salt-master -l debug Opening up another terminal and exec into the salt master we can see through the debug screen all of the data which is passed back and forth through the ZeroMQ bus. #Salt master cli salt 'base_lab_Leaf2' net.cli 'show version' #Salt master debug [DEBUG ] Sending event: tag = salt/job/20201027182445049986/new; data = {'jid': '20201027182445049986', 'tgt_type': 'glob', 'tgt': 'base_lab_Leaf2', 'user': 'root', 'fun': 'net.cli', 'arg': ['show version'], 'minions': ['base_lab_Leaf2'], 'missing': [], '_stamp': '2020-10-27T18:24:45.055000'} #Salt minion sending data back through the bus. [DEBUG ] Sending event: tag = salt/job/20201027182445049986/ret/base_lab_Leaf2; data = {'cmd': '_return', 'id': 'base_lab_Leaf2', 'success': True, 'return': {'out': {'show version': ' cEOSLab\\nHardware version: \\nSerial number: \\nSystem MAC address: 0242.c083.e816\\n\\nSoftware image version: 4.23.2F\\nArchitecture: i686\\nInternal build version: 4.23.2F-15405360.4232F\\nInternal build ID: 4cde5c53-3642-4934-8bcc-05691ffd79b3\\n\\ncEOS tools version: 1.1\\n\\nUptime: 0 weeks, 5 days, 18 hours and 16 minutes\\nTotal memory: 32970568 kB\\nFree memory: 18146044 kB\\n\\n'}, 'result': True, 'comment': ''}, 'retcode': 0, 'jid': '20201027182445049986', 'fun': 'net.cli', 'fun_args': ['show version'], '_stamp': '2020-10-27T18:24:45.484923'} What this means is anything can happen through the salt bus and we can react to this we will later get to reactors. This also means that to render templates or use certain data we can pull this from the salt master.","title":"ZeroMQ bus"},{"location":"bus/#stop-the-master-and-put-it-in-debug-mode","text":"service salt-master stop","title":"Stop the master and put it in debug mode."},{"location":"bus/#debug-the-salt-master","text":"salt-master -l debug Opening up another terminal and exec into the salt master we can see through the debug screen all of the data which is passed back and forth through the ZeroMQ bus. #Salt master cli salt 'base_lab_Leaf2' net.cli 'show version' #Salt master debug [DEBUG ] Sending event: tag = salt/job/20201027182445049986/new; data = {'jid': '20201027182445049986', 'tgt_type': 'glob', 'tgt': 'base_lab_Leaf2', 'user': 'root', 'fun': 'net.cli', 'arg': ['show version'], 'minions': ['base_lab_Leaf2'], 'missing': [], '_stamp': '2020-10-27T18:24:45.055000'} #Salt minion sending data back through the bus. [DEBUG ] Sending event: tag = salt/job/20201027182445049986/ret/base_lab_Leaf2; data = {'cmd': '_return', 'id': 'base_lab_Leaf2', 'success': True, 'return': {'out': {'show version': ' cEOSLab\\nHardware version: \\nSerial number: \\nSystem MAC address: 0242.c083.e816\\n\\nSoftware image version: 4.23.2F\\nArchitecture: i686\\nInternal build version: 4.23.2F-15405360.4232F\\nInternal build ID: 4cde5c53-3642-4934-8bcc-05691ffd79b3\\n\\ncEOS tools version: 1.1\\n\\nUptime: 0 weeks, 5 days, 18 hours and 16 minutes\\nTotal memory: 32970568 kB\\nFree memory: 18146044 kB\\n\\n'}, 'result': True, 'comment': ''}, 'retcode': 0, 'jid': '20201027182445049986', 'fun': 'net.cli', 'fun_args': ['show version'], '_stamp': '2020-10-27T18:24:45.484923'} What this means is anything can happen through the salt bus and we can react to this we will later get to reactors. This also means that to render templates or use certain data we can pull this from the salt master.","title":"Debug the salt master"},{"location":"cherryapi/","text":"Salt offers a very easy to use API based off of cherrypy Master file. The api itself requires some small configuration within the master file. external_auth: pam: saltdev: - .* rest_cherrypy: port: 8999 ssl_crt: /etc/pki/tls/certs/localhost.crt ssl_key: /etc/pki/tls/certs/localhost.key Create the ssl cert for the master. Afterwards the cherrypy ssl certs have to be created. salt-call --local tls.create_self_signed_cert Restart the salt-api service salt-api start Usage of the salt api The salt API is very simple. The idea is that you can pass in any salt information you would generally do via the salt cli through the API. First step is authenticate and receive a token. Second step is to use the token and send salt commands. Example script. #/srv/salt/api/.py #!/usr/bin/env python3.6 import requests import json import os import subprocess salt_ip = '127.0.0.1' port = 8999 user = 'saltdev' password = 'saltdev' requests.packages.urllib3.disable_warnings() def get_token(): headers = {'Content-type':'application/json'} urltoken = ('https://{}:{}/login').format(salt_ip, port) data = {\"username\":str(user), \"password\":str(password), \"eauth\":str('pam')} json_data = json.dumps(data) r = requests.post(urltoken, headers=headers, verify=False, data=json_data) r_data = (json.loads(r.content)) return(r_data['return'][0]['token']) def make_request(token): headers = {'Content-type':'application/json', 'X-Auth-Token': token} restcall = ('https://{}:{}').format(salt_ip, port) data = {\"client\":str(\"local\"), \"tgt\":str(\"*\"), \"fun\":str(\"net.arp\")} json_data = json.dumps(data) r = requests.post(restcall, headers=headers, verify=False, data=json_data) r_data = (json.loads(r.content)) print(json.dumps(r_data['return'][0], indent=4)) def main(): make_request(get_token()) if __name__ == '__main__': main() Running the script The output of this script will then provide arp outputs for each device. root@a8519e13fd15:/srv/salt# python3.6 api.py { \"base_lab_Spine2\": { \"out\": [ { \"interface\": \"Ethernet1\", \"mac\": \"02:42:C0:A8:A0:03\", \"ip\": \"10.1.0.1\", \"age\": 3262.0 } ], \"result\": true, \"comment\": \"\" }, \"base_lab_Spine1\": { \"out\": [], \"result\": true, \"comment\": \"\" }, \"base_lab_Leaf1\": { \"out\": [ { \"interface\": \"Ethernet1\", \"mac\": \"02:42:C0:2E:A3:34\", \"ip\": \"10.0.0.2\", \"age\": 4128.0 }, { \"interface\": \"Ethernet2\", \"mac\": \"02:42:C0:A8:A0:02\", \"ip\": \"10.1.0.2\", \"age\": 3263.0 } ], \"result\": true, \"comment\": \"\" }, \"base_lab_Leaf2\": { \"out\": [], \"result\": true, \"comment\": \"\" } }","title":"Salt api"},{"location":"cherryapi/#master-file","text":"The api itself requires some small configuration within the master file. external_auth: pam: saltdev: - .* rest_cherrypy: port: 8999 ssl_crt: /etc/pki/tls/certs/localhost.crt ssl_key: /etc/pki/tls/certs/localhost.key","title":"Master file."},{"location":"cherryapi/#create-the-ssl-cert-for-the-master","text":"Afterwards the cherrypy ssl certs have to be created. salt-call --local tls.create_self_signed_cert","title":"Create the ssl cert for the master."},{"location":"cherryapi/#restart-the-salt-api","text":"service salt-api start","title":"Restart the salt-api"},{"location":"cherryapi/#usage-of-the-salt-api","text":"The salt API is very simple. The idea is that you can pass in any salt information you would generally do via the salt cli through the API. First step is authenticate and receive a token. Second step is to use the token and send salt commands.","title":"Usage of the salt api"},{"location":"cherryapi/#example-script","text":"#/srv/salt/api/.py #!/usr/bin/env python3.6 import requests import json import os import subprocess salt_ip = '127.0.0.1' port = 8999 user = 'saltdev' password = 'saltdev' requests.packages.urllib3.disable_warnings() def get_token(): headers = {'Content-type':'application/json'} urltoken = ('https://{}:{}/login').format(salt_ip, port) data = {\"username\":str(user), \"password\":str(password), \"eauth\":str('pam')} json_data = json.dumps(data) r = requests.post(urltoken, headers=headers, verify=False, data=json_data) r_data = (json.loads(r.content)) return(r_data['return'][0]['token']) def make_request(token): headers = {'Content-type':'application/json', 'X-Auth-Token': token} restcall = ('https://{}:{}').format(salt_ip, port) data = {\"client\":str(\"local\"), \"tgt\":str(\"*\"), \"fun\":str(\"net.arp\")} json_data = json.dumps(data) r = requests.post(restcall, headers=headers, verify=False, data=json_data) r_data = (json.loads(r.content)) print(json.dumps(r_data['return'][0], indent=4)) def main(): make_request(get_token()) if __name__ == '__main__': main()","title":"Example script."},{"location":"cherryapi/#running-the-script","text":"The output of this script will then provide arp outputs for each device. root@a8519e13fd15:/srv/salt# python3.6 api.py { \"base_lab_Spine2\": { \"out\": [ { \"interface\": \"Ethernet1\", \"mac\": \"02:42:C0:A8:A0:03\", \"ip\": \"10.1.0.1\", \"age\": 3262.0 } ], \"result\": true, \"comment\": \"\" }, \"base_lab_Spine1\": { \"out\": [], \"result\": true, \"comment\": \"\" }, \"base_lab_Leaf1\": { \"out\": [ { \"interface\": \"Ethernet1\", \"mac\": \"02:42:C0:2E:A3:34\", \"ip\": \"10.0.0.2\", \"age\": 4128.0 }, { \"interface\": \"Ethernet2\", \"mac\": \"02:42:C0:A8:A0:02\", \"ip\": \"10.1.0.2\", \"age\": 3263.0 } ], \"result\": true, \"comment\": \"\" }, \"base_lab_Leaf2\": { \"out\": [], \"result\": true, \"comment\": \"\" } }","title":"Running the script"},{"location":"filestructure/","text":"Salt typically applies a best practices rule that puts all of the salt structure within the /srv/salt directory similar to this repo. |-- _grains |-- pillar |-- reactors |-- states |-- templates _grains Grains are any custom grains which are sent from the salt master to the salt minions. pillar Pillars are secrets which are stored as variables. For example, to start the salt proxy minion the salt proxy minion has to know the switch it needs to proxy to as well as a username/password. reactors Reactors are exactly what they sound like. Anything which is sent back and forth within the salt ZeroMQ bus you can react to. So for example, salt often includes Key value pairs within the ZeroMQ bus. You can key off of anything you want and send the data to a external system. So if you have a switch which is provsioning you can then react on that to tell a external system say slack to tell the channel the switch is currently provisioning. states States are exactly what they sound like. A state is the state of a given salt minion. For example, a state might be a one time configuration to configure a server for things like apache and mysql. In our lab there is a state to render configuration and a state to then push the configuration. States are a deterministic way to push a state to a salt minion. templates Templates hold templates of certain devices. In all of our examples templates hold the jinja2 templates for each of the configurations. .sls extensions Salt uses the .sls extension whithin any salt file. So for example, any of the state files will end with a .sls for example /srv/salt/states/push_vlans.sls which is a state module to push vlans to a device with the pyeapi module . Salt has methods in which is finds its state files in which it calls the file roots which is configerable via the salt master file which in our docker salt container can be found within the /etc/salt/master. There is also pillar_roots which is the same concept of where to check for pillars. file_roots: base: - /srv/salt - /srv/salt/states - /srv/salt/pillar - /srv/salt/states - /srv/salt/templates/ - /srv/salt/reactor/ pillar_roots: base: - /srv/salt - /srv/salt/templates - /srv/salt/pillar - /srv/salt/states Salt in all actuallity is just a giant file server on the master serving up files to the minions over the ZeroMQ bus and rendering each one to the minions. top.sls & pillar items Salt uses the concept of top.sls which is more or less a manifest for each minion and what states or items each minions will receive. We will get into that on the next portion talking about proxy minions. A top example using general servers would be as follows. #top.sls base: '*' - apache 'os:Fedora': - match: grain - mysql #apache.sls apache: pkg: - installed #mysql.sls mysql: pkg: - installed In the following example everything denoted with the * will recieve the apache package. Only the operating systems with Fedora will receive mysql and apache. So if we were to have a minion with any form of Debian/Ubuntu it would not receive the mysql package.","title":"Salt File structure"},{"location":"filestructure/#_grains","text":"Grains are any custom grains which are sent from the salt master to the salt minions.","title":"_grains"},{"location":"filestructure/#pillar","text":"Pillars are secrets which are stored as variables. For example, to start the salt proxy minion the salt proxy minion has to know the switch it needs to proxy to as well as a username/password.","title":"pillar"},{"location":"filestructure/#reactors","text":"Reactors are exactly what they sound like. Anything which is sent back and forth within the salt ZeroMQ bus you can react to. So for example, salt often includes Key value pairs within the ZeroMQ bus. You can key off of anything you want and send the data to a external system. So if you have a switch which is provsioning you can then react on that to tell a external system say slack to tell the channel the switch is currently provisioning.","title":"reactors"},{"location":"filestructure/#states","text":"States are exactly what they sound like. A state is the state of a given salt minion. For example, a state might be a one time configuration to configure a server for things like apache and mysql. In our lab there is a state to render configuration and a state to then push the configuration. States are a deterministic way to push a state to a salt minion.","title":"states"},{"location":"filestructure/#templates","text":"Templates hold templates of certain devices. In all of our examples templates hold the jinja2 templates for each of the configurations.","title":"templates"},{"location":"filestructure/#sls-extensions","text":"Salt uses the .sls extension whithin any salt file. So for example, any of the state files will end with a .sls for example /srv/salt/states/push_vlans.sls which is a state module to push vlans to a device with the pyeapi module . Salt has methods in which is finds its state files in which it calls the file roots which is configerable via the salt master file which in our docker salt container can be found within the /etc/salt/master. There is also pillar_roots which is the same concept of where to check for pillars. file_roots: base: - /srv/salt - /srv/salt/states - /srv/salt/pillar - /srv/salt/states - /srv/salt/templates/ - /srv/salt/reactor/ pillar_roots: base: - /srv/salt - /srv/salt/templates - /srv/salt/pillar - /srv/salt/states Salt in all actuallity is just a giant file server on the master serving up files to the minions over the ZeroMQ bus and rendering each one to the minions.","title":".sls extensions"},{"location":"filestructure/#topsls-pillar-items","text":"Salt uses the concept of top.sls which is more or less a manifest for each minion and what states or items each minions will receive. We will get into that on the next portion talking about proxy minions. A top example using general servers would be as follows. #top.sls base: '*' - apache 'os:Fedora': - match: grain - mysql #apache.sls apache: pkg: - installed #mysql.sls mysql: pkg: - installed In the following example everything denoted with the * will recieve the apache package. Only the operating systems with Fedora will receive mysql and apache. So if we were to have a minion with any form of Debian/Ubuntu it would not receive the mysql package.","title":"top.sls &amp; pillar items"},{"location":"formulas/","text":"","title":"Formulas"},{"location":"grains/","text":"Salt grains are meta data about a salt minion in which you can use to etheir render configs or target devices based off things like different operating systems or ethernet nics. showing grains. #Salt master salt 'base_lab_Leaf2' grains.items base_lab_Leaf2: ---------- cpuarch: x86_64 cwd: / dns: ---------- domain: ip4_nameservers: - 127.0.0.11 ip6_nameservers: nameservers: - 127.0.0.11 options: - ndots:0 search: sortlist: fqdns: - a8519e13fd15 gpus: host: base_lab_Leaf2 hostname: leaf2 hwaddr_interfaces: ---------- eth0: 02:42:ac:19:00:02 id: base_lab_Leaf2 interfaces: - Ethernet1 - Ethernet2 - Ethernet3 - Ethernet4 - Loopback0 - Loopback1 Targeting minions based off of grains. Targeting all minions that have eos as a OS. salt -G 'os:eos' cmd.run 'uname' base_lab_Leaf2: Linux base_lab_Leaf1: Linux base_lab_Spine1: Linux base_lab_Spine2: Linux","title":"Grains"},{"location":"grains/#showing-grains","text":"#Salt master salt 'base_lab_Leaf2' grains.items base_lab_Leaf2: ---------- cpuarch: x86_64 cwd: / dns: ---------- domain: ip4_nameservers: - 127.0.0.11 ip6_nameservers: nameservers: - 127.0.0.11 options: - ndots:0 search: sortlist: fqdns: - a8519e13fd15 gpus: host: base_lab_Leaf2 hostname: leaf2 hwaddr_interfaces: ---------- eth0: 02:42:ac:19:00:02 id: base_lab_Leaf2 interfaces: - Ethernet1 - Ethernet2 - Ethernet3 - Ethernet4 - Loopback0 - Loopback1","title":"showing grains."},{"location":"grains/#targeting-minions-based-off-of-grains","text":"Targeting all minions that have eos as a OS. salt -G 'os:eos' cmd.run 'uname' base_lab_Leaf2: Linux base_lab_Leaf1: Linux base_lab_Spine1: Linux base_lab_Spine2: Linux","title":"Targeting minions based off of grains."},{"location":"hello-world/","text":"About this guided hello world The content of this guided hello world is available in this reposiroy https://github.com/arista-netdevops-community/saltstack-hello-world In this hello world, SaltStack is running in one single container. The SaltStack content of this hello world demo has been designed for one single SaltStack container. How to use this hello world demo Clone the repository git clone https://github.com/arista-netdevops-community/saltstack-hello-world.git Move to the local repository cd saltstack-hello-world Build an image from the Dockerfile docker build --tag salt_eos:1.5 . List images and verify docker images | grep salt_eos Update the SaltStack pillar Update the pillar with your devices IP/username/password Create a container docker run -d -t --rm --name salt \\ -p 5001:5001 -p 4505:4505 -p 4506:4506 \\ -v $PWD/master:/etc/salt/master \\ -v $PWD/proxy:/etc/salt/proxy \\ -v $PWD/minion:/etc/salt/minion \\ -v $PWD/pillar/.:/srv/pillar/. \\ -v $PWD/states/.:/srv/salt/states/. \\ -v $PWD/templates/.:/srv/salt/templates/. \\ -v $PWD/eos/.:/srv/salt/eos \\ -v $PWD/_modules/.:/srv/salt/_modules/. \\ salt_eos:1.5 List containers and verify docker ps | grep salt Move to the container docker exec -it salt bash SaltStack configuration directory and configuration files SaltStack default configuration directory ls /etc/salt/ Using the above docker run command: master configuration file more /etc/salt/master proxy configuration file more /etc/salt/proxy minion configuration file more /etc/salt/minion Start salt-master and salt-minion This can be done: - using the python script start_saltstack.py from the host - or manually from the container using - Ubuntu services - or SaltStack command-line Using python from the host python3 start_saltstack.py Or using Ubuntu services from the container List all the services service --status-all we can use start/stop/restart/status. service salt-master start service salt-master status service salt-minion start service salt-minion status Or using SaltStack command-line from the container Start as a daemon (in background) salt-master -d salt-minion -d ps -ef | grep salt Start a salt-proxy daemon for each device If you did not use the python script start_saltstack.py you also need to start a salt-proxy daemon for each device salt-proxy --proxyid=leaf1 -d salt-proxy --proxyid=leaf2 -d salt-proxy --proxyid=spine1 -d salt-proxy --proxyid=spine2 -d ps -ef | grep proxy Check if the keys are accepted Help salt-key --help To list all keys salt-key -L Run this command to accept one pending key salt-key -a minion1 -y Run this command to accept all pending keys salt-key -A -y Or use this in the master configuration file to auto accept keys auto_accept: True Test if the minion and proxies are up and responding to the master It is not an ICMP ping salt minion1 test.ping salt leaf1 test.ping salt '*' test.ping Grains module usage examples salt 'leaf1' grains.items salt 'leaf1' grains.ls salt 'leaf1' grains.item os vendor version host Pillar module usage examples salt 'leaf1' pillar.ls salt 'leaf1' pillar.items salt 'leaf1' pillar.get pyeapi salt 'leaf1' pillar.item pyeapi vlans About SaltStack targeting system It is very flexible. Using list salt -L \"minion1, leaf1\" test.ping Using regex salt \"leaf*\" test.ping salt '*' test.ping Using grains salt -G 'os:eos' test.ping salt -G 'os:eos' cmd.run 'uname' salt -G 'os:eos' net.cli 'show version' Using nodegroups Include this in the master configuration file: nodegroups: leaves: 'L@leaf1,leaf2' spines: - spine1 - spine2 eos: 'G@os:eos' salt -N eos test.ping salt -N leaves test.ping salt -N spines test.ping About SaltStack modules List modules salt 'leaf1' sys.list_modules 'napalm*' List the functions for a module salt 'leaf1' sys.list_functions net salt 'leaf1' sys.list_functions napalm salt 'leaf1' sys.list_functions napalm_net net and napalm_net is the same module. Get the documentation for a module Example with Napalm salt 'leaf1' sys.doc net salt 'leaf1' sys.doc net.traceroute or salt 'leaf1' net -d salt 'leaf1' net.traceroute -d About templates Check if a template renders The file vlans.j2 is in the master file server salt '*' slsutil.renderer salt://vlans.j2 'jinja' Render a template The file render.sls and the file vlans.j2 are in the master file server salt -G 'os:eos' state.sls render ls /srv/salt/eos/*cfg Napalm proxy usage examples This repository uses the Napalm proxy Napalm proxy source code Pillar example for Napalm proxy ( pillar/leaf1.sls ): proxy: proxytype: napalm driver: eos host: 10.73.1.105 username: ansible password: ansible The Napalm proxy uses different modules to interact with network devices. net module net and napalm_net is the same module. net module source code Examples: we can use the net or napalm.net commands: salt 'leaf*' net.load_config text='vlan 8' test=True The file vlan.cfg is available in the master file server salt 'leaf*' net.load_config filename='salt://vlan.cfg' test=True salt 'leaf*' net.cli 'show version' 'show vlan' salt 'leaf1' net.cli 'show vlan | json' salt 'leaf1' net.cli 'show version' --out=json salt 'leaf1' net.cli 'show version' --output=json salt 'leaf1' net.cli 'show vlan' --output-file=show_vlan.txt salt 'leaf1' net.cli 'show version' > show_version.txt salt 'leaf1' net.lldp salt 'leaf1' net.lldp interface='Ethernet1' salt 'leaf1' net.arp salt 'leaf1' net.connected salt 'leaf1' net.facts salt 'leaf1' net.interfaces salt 'leaf1' net.ipaddrs salt 'leaf1' net.config source=running --output-file=leaf1_running.cfg napalm module napalm module source code Examples: salt 'leaf1' napalm.alive salt 'leaf1' napalm.pyeapi_run_commands 'show version' encoding=json salt 'leaf1' napalm.pyeapi_run_commands 'show version' --out=raw salt 'leaf1' napalm.pyeapi_run_commands 'show version' --out=json napalm.pyeapi_run_commands forwards to pyeapi.run_commands Netmiko proxy usage examples The Netmiko execution module can be used with a Netmiko proxy Netmiko execution module source code Netmiko proxy source code This repository uses the Napalm proxy. You can replace it with a Netmiko proxy. Here's an example of pillar for Netmiko proxy: proxy: proxytype: netmiko device_type: arista_eos host: spine1 ip: 10.73.1.101 username: ansible password: ansible Examples: salt '*' netmiko.send_command -d salt 'spine1' netmiko.send_command 'show version' pyeapi execution module usage examples The pyeapi execution module can be used to interact with Arista switches. It is flexible enough to execute the commands both when running under an pyeapi Proxy, as well as running under a Regular Minion by specifying the connection arguments, i.e., host , username , password transport etc. Examples: salt 'leaf1' pyeapi.run_commands 'show version' salt 'leaf1' pyeapi.get_config as_string=True How to run pyeapi execution module in a sls file To collect show commands salt -G 'os:eos' state.sls collect_commands ls /tmp/*/*.json To configure devices with a template The file push_vlans.sls and the file vlans.j2 are in the master file server salt 'leaf1' state.sls push_vlans or salt 'leaf1' state.apply push_vlans Verify: salt 'leaf1' net.cli 'show vlan' To configure devices with a file The file render.sls and the file vlans.j2 are in the master file server salt -G 'os:eos' state.sls render ls /srv/salt/eos/*cfg The file push_config.sls is in the master file server salt -G 'os:eos' state.sls push_config Writing Execution Modules A Salt execution module is a Python module placed in a directory called _modules at the root of the Salt fileserver. In this setup the directory _modules is /srv/salt/_modules The execution module _modules/custom_eos.py is /srv/salt/_modules/custom_eos.py salt 'leaf1' custom_eos.version salt 'leaf1' custom_eos.model If you create a new execution module, run this command to sync execution modules placed in the _modules directory: salt '*' saltutil.sync_modules After loading the modules, you can use them Troubleshooting SaltStack Source code https://github.com/saltstack/salt Move to the container docker exec -it salt bash Print basic information about the operating system uname -a lsb_release -a List the installed python packages pip3 list pip3 freeze Check the SaltStack Version salt --versions-report salt --version salt-master --version salt-minion --version salt-proxy --version SaltStack help salt --help Verbose Use -v to also display the job id: salt 'leaf1' net.cli 'show version' 'show vlan' -v Start SaltStack in foreground with a debug log level salt-master -l debug salt-minion -l debug salt-proxy --proxyid=leaf1 -l debug ps -ef | grep salt Check log more /var/log/salt/master more /var/log/salt/proxy more /var/log/salt/minion tail -f /var/log/salt/master To kill a process ps -ef | grep salt kill PID tcpdump run this command on the master if you need to display received packets tcpdump -i < interface > port < port > -vv Example tcpdump -i eth0 port 5001 -vv Watch the event bus run this command on the master if you need to watch the event bus: salt-run state.event pretty=True run this command to fire an event: salt \"minion1\" event.fire_master '{\"data\": \"message to be sent in the event\"}' 'tag/blabla' Check port connectivity From outside the container, check port connectivity with the nc command: From the host where the container runs: nc -v -z < salt_container_ip > 4505 nc -v -z < salt_container_ip > 4506 Example if the container ip is 172.17.0.2: nc -v -z 172.17.0.2 4505 nc -v -z 172.17.0.2 4506 From another host: nc -v -z < host_that_has_the_container > 4505 nc -v -z < host_that_has_the_container > 4506 Example if the host ip where the container runs is 10.83.28.180: nc -v -z 10.83.28.180 4505 nc -v -z 10.83.28.180 4506","title":"Hello world guided demo"},{"location":"hello-world/#about-this-guided-hello-world","text":"The content of this guided hello world is available in this reposiroy https://github.com/arista-netdevops-community/saltstack-hello-world In this hello world, SaltStack is running in one single container. The SaltStack content of this hello world demo has been designed for one single SaltStack container.","title":"About this guided hello world"},{"location":"hello-world/#how-to-use-this-hello-world-demo","text":"","title":"How to use this hello world demo"},{"location":"hello-world/#clone-the-repository","text":"git clone https://github.com/arista-netdevops-community/saltstack-hello-world.git","title":"Clone the repository"},{"location":"hello-world/#move-to-the-local-repository","text":"cd saltstack-hello-world","title":"Move to the local repository"},{"location":"hello-world/#build-an-image-from-the-dockerfile","text":"docker build --tag salt_eos:1.5 . List images and verify docker images | grep salt_eos","title":"Build an image from the Dockerfile"},{"location":"hello-world/#update-the-saltstack-pillar","text":"Update the pillar with your devices IP/username/password","title":"Update the SaltStack pillar"},{"location":"hello-world/#create-a-container","text":"docker run -d -t --rm --name salt \\ -p 5001:5001 -p 4505:4505 -p 4506:4506 \\ -v $PWD/master:/etc/salt/master \\ -v $PWD/proxy:/etc/salt/proxy \\ -v $PWD/minion:/etc/salt/minion \\ -v $PWD/pillar/.:/srv/pillar/. \\ -v $PWD/states/.:/srv/salt/states/. \\ -v $PWD/templates/.:/srv/salt/templates/. \\ -v $PWD/eos/.:/srv/salt/eos \\ -v $PWD/_modules/.:/srv/salt/_modules/. \\ salt_eos:1.5 List containers and verify docker ps | grep salt","title":"Create a container"},{"location":"hello-world/#move-to-the-container","text":"docker exec -it salt bash","title":"Move to the container"},{"location":"hello-world/#saltstack-configuration-directory-and-configuration-files","text":"SaltStack default configuration directory ls /etc/salt/ Using the above docker run command: master configuration file more /etc/salt/master proxy configuration file more /etc/salt/proxy minion configuration file more /etc/salt/minion","title":"SaltStack configuration directory and configuration files"},{"location":"hello-world/#start-salt-master-and-salt-minion","text":"This can be done: - using the python script start_saltstack.py from the host - or manually from the container using - Ubuntu services - or SaltStack command-line","title":"Start salt-master and salt-minion"},{"location":"hello-world/#using-python-from-the-host","text":"python3 start_saltstack.py","title":"Using python from the host"},{"location":"hello-world/#or-using-ubuntu-services-from-the-container","text":"List all the services service --status-all we can use start/stop/restart/status. service salt-master start service salt-master status service salt-minion start service salt-minion status","title":"Or using Ubuntu services from the container"},{"location":"hello-world/#or-using-saltstack-command-line-from-the-container","text":"Start as a daemon (in background) salt-master -d salt-minion -d ps -ef | grep salt","title":"Or using SaltStack command-line from the container"},{"location":"hello-world/#start-a-salt-proxy-daemon-for-each-device","text":"If you did not use the python script start_saltstack.py you also need to start a salt-proxy daemon for each device salt-proxy --proxyid=leaf1 -d salt-proxy --proxyid=leaf2 -d salt-proxy --proxyid=spine1 -d salt-proxy --proxyid=spine2 -d ps -ef | grep proxy","title":"Start a salt-proxy daemon for each device"},{"location":"hello-world/#check-if-the-keys-are-accepted","text":"Help salt-key --help To list all keys salt-key -L Run this command to accept one pending key salt-key -a minion1 -y Run this command to accept all pending keys salt-key -A -y Or use this in the master configuration file to auto accept keys auto_accept: True","title":"Check if the keys are accepted"},{"location":"hello-world/#test-if-the-minion-and-proxies-are-up-and-responding-to-the-master","text":"It is not an ICMP ping salt minion1 test.ping salt leaf1 test.ping salt '*' test.ping","title":"Test if the minion and proxies are up and responding to the master"},{"location":"hello-world/#grains-module-usage-examples","text":"salt 'leaf1' grains.items salt 'leaf1' grains.ls salt 'leaf1' grains.item os vendor version host","title":"Grains module usage examples"},{"location":"hello-world/#pillar-module-usage-examples","text":"salt 'leaf1' pillar.ls salt 'leaf1' pillar.items salt 'leaf1' pillar.get pyeapi salt 'leaf1' pillar.item pyeapi vlans","title":"Pillar module usage examples"},{"location":"hello-world/#about-saltstack-targeting-system","text":"It is very flexible.","title":"About SaltStack targeting system"},{"location":"hello-world/#using-list","text":"salt -L \"minion1, leaf1\" test.ping","title":"Using list"},{"location":"hello-world/#using-regex","text":"salt \"leaf*\" test.ping salt '*' test.ping","title":"Using regex"},{"location":"hello-world/#using-grains","text":"salt -G 'os:eos' test.ping salt -G 'os:eos' cmd.run 'uname' salt -G 'os:eos' net.cli 'show version'","title":"Using grains"},{"location":"hello-world/#using-nodegroups","text":"Include this in the master configuration file: nodegroups: leaves: 'L@leaf1,leaf2' spines: - spine1 - spine2 eos: 'G@os:eos' salt -N eos test.ping salt -N leaves test.ping salt -N spines test.ping","title":"Using nodegroups"},{"location":"hello-world/#about-saltstack-modules","text":"","title":"About SaltStack modules"},{"location":"hello-world/#list-modules","text":"salt 'leaf1' sys.list_modules 'napalm*'","title":"List modules"},{"location":"hello-world/#list-the-functions-for-a-module","text":"salt 'leaf1' sys.list_functions net salt 'leaf1' sys.list_functions napalm salt 'leaf1' sys.list_functions napalm_net net and napalm_net is the same module.","title":"List the functions for a module"},{"location":"hello-world/#get-the-documentation-for-a-module","text":"Example with Napalm salt 'leaf1' sys.doc net salt 'leaf1' sys.doc net.traceroute or salt 'leaf1' net -d salt 'leaf1' net.traceroute -d","title":"Get the documentation for a module"},{"location":"hello-world/#about-templates","text":"","title":"About templates"},{"location":"hello-world/#check-if-a-template-renders","text":"The file vlans.j2 is in the master file server salt '*' slsutil.renderer salt://vlans.j2 'jinja'","title":"Check if a template renders"},{"location":"hello-world/#render-a-template","text":"The file render.sls and the file vlans.j2 are in the master file server salt -G 'os:eos' state.sls render ls /srv/salt/eos/*cfg","title":"Render a template"},{"location":"hello-world/#napalm-proxy-usage-examples","text":"This repository uses the Napalm proxy Napalm proxy source code Pillar example for Napalm proxy ( pillar/leaf1.sls ): proxy: proxytype: napalm driver: eos host: 10.73.1.105 username: ansible password: ansible The Napalm proxy uses different modules to interact with network devices.","title":"Napalm proxy usage examples"},{"location":"hello-world/#net-module","text":"net and napalm_net is the same module. net module source code Examples: we can use the net or napalm.net commands: salt 'leaf*' net.load_config text='vlan 8' test=True The file vlan.cfg is available in the master file server salt 'leaf*' net.load_config filename='salt://vlan.cfg' test=True salt 'leaf*' net.cli 'show version' 'show vlan' salt 'leaf1' net.cli 'show vlan | json' salt 'leaf1' net.cli 'show version' --out=json salt 'leaf1' net.cli 'show version' --output=json salt 'leaf1' net.cli 'show vlan' --output-file=show_vlan.txt salt 'leaf1' net.cli 'show version' > show_version.txt salt 'leaf1' net.lldp salt 'leaf1' net.lldp interface='Ethernet1' salt 'leaf1' net.arp salt 'leaf1' net.connected salt 'leaf1' net.facts salt 'leaf1' net.interfaces salt 'leaf1' net.ipaddrs salt 'leaf1' net.config source=running --output-file=leaf1_running.cfg","title":"net module"},{"location":"hello-world/#napalm-module","text":"napalm module source code Examples: salt 'leaf1' napalm.alive salt 'leaf1' napalm.pyeapi_run_commands 'show version' encoding=json salt 'leaf1' napalm.pyeapi_run_commands 'show version' --out=raw salt 'leaf1' napalm.pyeapi_run_commands 'show version' --out=json napalm.pyeapi_run_commands forwards to pyeapi.run_commands","title":"napalm module"},{"location":"hello-world/#netmiko-proxy-usage-examples","text":"The Netmiko execution module can be used with a Netmiko proxy Netmiko execution module source code Netmiko proxy source code This repository uses the Napalm proxy. You can replace it with a Netmiko proxy. Here's an example of pillar for Netmiko proxy: proxy: proxytype: netmiko device_type: arista_eos host: spine1 ip: 10.73.1.101 username: ansible password: ansible Examples: salt '*' netmiko.send_command -d salt 'spine1' netmiko.send_command 'show version'","title":"Netmiko proxy usage examples"},{"location":"hello-world/#pyeapi-execution-module-usage-examples","text":"The pyeapi execution module can be used to interact with Arista switches. It is flexible enough to execute the commands both when running under an pyeapi Proxy, as well as running under a Regular Minion by specifying the connection arguments, i.e., host , username , password transport etc. Examples: salt 'leaf1' pyeapi.run_commands 'show version' salt 'leaf1' pyeapi.get_config as_string=True","title":"pyeapi execution module usage examples"},{"location":"hello-world/#how-to-run-pyeapi-execution-module-in-a-sls-file","text":"","title":"How to run pyeapi execution module in a sls file"},{"location":"hello-world/#to-collect-show-commands","text":"salt -G 'os:eos' state.sls collect_commands ls /tmp/*/*.json","title":"To collect show commands"},{"location":"hello-world/#to-configure-devices-with-a-template","text":"The file push_vlans.sls and the file vlans.j2 are in the master file server salt 'leaf1' state.sls push_vlans or salt 'leaf1' state.apply push_vlans Verify: salt 'leaf1' net.cli 'show vlan'","title":"To configure devices with a template"},{"location":"hello-world/#to-configure-devices-with-a-file","text":"The file render.sls and the file vlans.j2 are in the master file server salt -G 'os:eos' state.sls render ls /srv/salt/eos/*cfg The file push_config.sls is in the master file server salt -G 'os:eos' state.sls push_config","title":"To configure devices with a file"},{"location":"hello-world/#writing-execution-modules","text":"A Salt execution module is a Python module placed in a directory called _modules at the root of the Salt fileserver. In this setup the directory _modules is /srv/salt/_modules The execution module _modules/custom_eos.py is /srv/salt/_modules/custom_eos.py salt 'leaf1' custom_eos.version salt 'leaf1' custom_eos.model If you create a new execution module, run this command to sync execution modules placed in the _modules directory: salt '*' saltutil.sync_modules After loading the modules, you can use them","title":"Writing Execution Modules"},{"location":"hello-world/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"hello-world/#saltstack-source-code","text":"https://github.com/saltstack/salt","title":"SaltStack Source code"},{"location":"hello-world/#move-to-the-container_1","text":"docker exec -it salt bash","title":"Move to the container"},{"location":"hello-world/#print-basic-information-about-the-operating-system","text":"uname -a lsb_release -a","title":"Print basic information about the operating system"},{"location":"hello-world/#list-the-installed-python-packages","text":"pip3 list pip3 freeze","title":"List the installed python packages"},{"location":"hello-world/#check-the-saltstack-version","text":"salt --versions-report salt --version salt-master --version salt-minion --version salt-proxy --version","title":"Check the SaltStack Version"},{"location":"hello-world/#saltstack-help","text":"salt --help","title":"SaltStack help"},{"location":"hello-world/#verbose","text":"Use -v to also display the job id: salt 'leaf1' net.cli 'show version' 'show vlan' -v","title":"Verbose"},{"location":"hello-world/#start-saltstack-in-foreground-with-a-debug-log-level","text":"salt-master -l debug salt-minion -l debug salt-proxy --proxyid=leaf1 -l debug ps -ef | grep salt","title":"Start SaltStack in foreground with a debug log level"},{"location":"hello-world/#check-log","text":"more /var/log/salt/master more /var/log/salt/proxy more /var/log/salt/minion tail -f /var/log/salt/master","title":"Check log"},{"location":"hello-world/#to-kill-a-process","text":"ps -ef | grep salt kill PID","title":"To kill a process"},{"location":"hello-world/#tcpdump","text":"run this command on the master if you need to display received packets tcpdump -i < interface > port < port > -vv Example tcpdump -i eth0 port 5001 -vv","title":"tcpdump"},{"location":"hello-world/#watch-the-event-bus","text":"run this command on the master if you need to watch the event bus: salt-run state.event pretty=True run this command to fire an event: salt \"minion1\" event.fire_master '{\"data\": \"message to be sent in the event\"}' 'tag/blabla'","title":"Watch the event bus"},{"location":"hello-world/#check-port-connectivity","text":"From outside the container, check port connectivity with the nc command: From the host where the container runs: nc -v -z < salt_container_ip > 4505 nc -v -z < salt_container_ip > 4506 Example if the container ip is 172.17.0.2: nc -v -z 172.17.0.2 4505 nc -v -z 172.17.0.2 4506 From another host: nc -v -z < host_that_has_the_container > 4505 nc -v -z < host_that_has_the_container > 4506 Example if the host ip where the container runs is 10.83.28.180: nc -v -z 10.83.28.180 4505 nc -v -z 10.83.28.180 4506","title":"Check port connectivity"},{"location":"install/","text":"To start the lab Import the ceoslab file docker import cEOS-lab-4.23.2F.tar.xz ceosimage:4.23.2f Create the docker salt container for this lab docker create --name salt -v $PWD/salt_files/:/srv/salt -p 8999:8999 --network=base_lab_net-0 burnyd/salt Install docker-topo pip install git+https://github.com/networkop/docker-topo.git Run docker topo and salt Docker container. docker-topo --create base.yml && docker-topo --create base_lab.yml && docker start salt This will create the infrastructure described within the base_lab.yml file Examining the lab docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 79d74ab70901 ceosimage:4.23.2F \"/sbin/init systemd.\u2026\" 28 seconds ago Up 25 seconds 0.0.0.0:8801->80/tcp, 0.0.0.0:9001->443/tcp, 0.0.0.0:7001->6030/tcp base_lab_Leaf2 0eeaa096e20e ceosimage:4.23.2F \"/sbin/init systemd.\u2026\" 31 seconds ago Up 28 seconds 0.0.0.0:8800->80/tcp, 0.0.0.0:9000->443/tcp, 0.0.0.0:7000->6030/tcp base_lab_Leaf1 296b86bea528 ceosimage:4.23.2F \"/sbin/init systemd.\u2026\" 33 seconds ago Up 31 seconds 0.0.0.0:8803->80/tcp, 0.0.0.0:9003->443/tcp, 0.0.0.0:7003->6030/tcp base_lab_Spine2 111f7ce06ede ceosimage:4.23.2F \"/sbin/init systemd.\u2026\" 35 seconds ago Up 33 seconds 0.0.0.0:8802->80/tcp, 0.0.0.0:9002->443/tcp, 0.0.0.0:7002->6030/tcp base_lab_Spine1 37a2dfecbdfb 70133de29164 \"/bin/sh -c 'tail -f\u2026\" 40 seconds ago Up 40 seconds ago 0.0.0.0:8999->8999/tcp salt Console into the lab Start the salt-minions and accept the keys. docker exec -it salt bash service salt-master start salt-proxy --proxyid=base_lab_Spine2 -d salt-proxy --proxyid=base_lab_Spine1 -d salt-proxy --proxyid=base_lab_Leaf1 -d salt-proxy --proxyid=base_lab_Leaf2 -d List the salt keys (salt-key -L) and accept the salt-keys from the minions (salt-key -A) salt-key -L Accepted Keys: Denied Keys: Unaccepted Keys: base_lab_Leaf1 base_lab_Leaf2 base_lab_Spine1 base_lab_Spine2 salt-key -A The following keys are going to be accepted: Unaccepted Keys: base_lab_Leaf1 base_lab_Leaf2 base_lab_Spine1 base_lab_Spine2 Provide a test ping for all minions (salt '*' test.ping) Proceed? [n/Y] Y Key for minion base_lab_Leaf1 accepted. Key for minion base_lab_Leaf2 accepted. Key for minion base_lab_Spine1 accepted. Key for minion base_lab_Spine2 accepted. salt '*' test.ping base_lab_Leaf2: True base_lab_Spine2: True base_lab_Spine1: True base_lab_Leaf1: True Destroy the lab docker-topo --destroy base.yml && docker-topo --create base_lab.yml && docker stop salt","title":"install"},{"location":"install/#to-start-the-lab","text":"","title":"To start the lab"},{"location":"install/#import-the-ceoslab-file","text":"docker import cEOS-lab-4.23.2F.tar.xz ceosimage:4.23.2f","title":"Import the ceoslab file"},{"location":"install/#create-the-docker-salt-container-for-this-lab","text":"docker create --name salt -v $PWD/salt_files/:/srv/salt -p 8999:8999 --network=base_lab_net-0 burnyd/salt","title":"Create the docker salt container for this lab"},{"location":"install/#install-docker-topo","text":"pip install git+https://github.com/networkop/docker-topo.git","title":"Install docker-topo"},{"location":"install/#run-docker-topo-and-salt-docker-container","text":"docker-topo --create base.yml && docker-topo --create base_lab.yml && docker start salt","title":"Run docker topo and salt Docker container."},{"location":"install/#this-will-create-the-infrastructure-described-within-the-base_labyml-file","text":"","title":"This will create the infrastructure described within the base_lab.yml file"},{"location":"install/#examining-the-lab","text":"docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 79d74ab70901 ceosimage:4.23.2F \"/sbin/init systemd.\u2026\" 28 seconds ago Up 25 seconds 0.0.0.0:8801->80/tcp, 0.0.0.0:9001->443/tcp, 0.0.0.0:7001->6030/tcp base_lab_Leaf2 0eeaa096e20e ceosimage:4.23.2F \"/sbin/init systemd.\u2026\" 31 seconds ago Up 28 seconds 0.0.0.0:8800->80/tcp, 0.0.0.0:9000->443/tcp, 0.0.0.0:7000->6030/tcp base_lab_Leaf1 296b86bea528 ceosimage:4.23.2F \"/sbin/init systemd.\u2026\" 33 seconds ago Up 31 seconds 0.0.0.0:8803->80/tcp, 0.0.0.0:9003->443/tcp, 0.0.0.0:7003->6030/tcp base_lab_Spine2 111f7ce06ede ceosimage:4.23.2F \"/sbin/init systemd.\u2026\" 35 seconds ago Up 33 seconds 0.0.0.0:8802->80/tcp, 0.0.0.0:9002->443/tcp, 0.0.0.0:7002->6030/tcp base_lab_Spine1 37a2dfecbdfb 70133de29164 \"/bin/sh -c 'tail -f\u2026\" 40 seconds ago Up 40 seconds ago 0.0.0.0:8999->8999/tcp salt","title":"Examining the lab"},{"location":"install/#console-into-the-lab-start-the-salt-minions-and-accept-the-keys","text":"docker exec -it salt bash service salt-master start salt-proxy --proxyid=base_lab_Spine2 -d salt-proxy --proxyid=base_lab_Spine1 -d salt-proxy --proxyid=base_lab_Leaf1 -d salt-proxy --proxyid=base_lab_Leaf2 -d","title":"Console into the lab Start the salt-minions and accept the keys."},{"location":"install/#list-the-salt-keys-salt-key-l-and-accept-the-salt-keys-from-the-minions-salt-key-a","text":"salt-key -L Accepted Keys: Denied Keys: Unaccepted Keys: base_lab_Leaf1 base_lab_Leaf2 base_lab_Spine1 base_lab_Spine2 salt-key -A The following keys are going to be accepted: Unaccepted Keys: base_lab_Leaf1 base_lab_Leaf2 base_lab_Spine1 base_lab_Spine2","title":"List the salt keys (salt-key -L) and accept the salt-keys from the minions (salt-key -A)"},{"location":"install/#provide-a-test-ping-for-all-minions-salt-testping","text":"Proceed? [n/Y] Y Key for minion base_lab_Leaf1 accepted. Key for minion base_lab_Leaf2 accepted. Key for minion base_lab_Spine1 accepted. Key for minion base_lab_Spine2 accepted. salt '*' test.ping base_lab_Leaf2: True base_lab_Spine2: True base_lab_Spine1: True base_lab_Leaf1: True","title":"Provide a test ping for all minions (salt '*' test.ping)"},{"location":"install/#destroy-the-lab","text":"docker-topo --destroy base.yml && docker-topo --create base_lab.yml && docker stop salt","title":"Destroy the lab"},{"location":"links/","text":"","title":"Salt Links"},{"location":"minions/","text":"Salt has two typical ways of deployment for network nodes minion nodes. Salt native minion - This is a minion install on the switch operating system itself in the form of a extension just like a linux server. Salt proxy minion - This is a service which will run either on the master or somewhere externally which will proxy for example, all api request to the switch back to the master and over the ZeroMQ bus. The reasoning for this is a lot of people do not like to run the actual salt minion on the network OS. If you recall all the minions work in a master/minion style of functionality. So the minion will start up discover the salt master and the master then has to accept the key from the minon. Salt discovery Upon a salt minion being installed it needs to find the salt master. The default way is having a dns lookup for the hostname of salt this is also configurable within the minion config file under /etc/salt/minion #/etc/salt/minion master: 1.2.3.4 The salt minion will then try to negotiate a secure connection to 1.2.3.4 once the connection is made the salt master will have to accept the connection. This can either be manual or automatic. The good part about salt versus other configuration management platforms is that salt does not require a username/password. In this example the salt minion is brought online. Once online it has to find its master. In a manual mode the master issues a 'salt-key -L' which will scan for new nodes. The 'salt-key -A' command will accept all new minion nodes. Salt native minion A salt native minion is the typical way of installing a linux minion that will sit on the device and communicate with the underlying host itself and send the data back to the master. At the time of writing this the native minion is almost GA and will be out at any point. Right now the minion will use either pyeapi or napalm to configure the device. Salt native minion Running the lab we typically start with salt-proxy --proxyid=base_lab_Leaf1 -d what this does is run a proxy process using the proxyid of base_lab_leaf1 salt-proxy itself will read the /srv/salt/pillars/top.sls file and find the proxies in which it needs to match base_lab_leaf1 for the proxy info. #/srv/salt/pillar/top.sls base: base_lab_Spine1: - base_lab_Spine1 base_lab_Spine2: - base_lab_Spine2 base_lab_Leaf1: - base_lab_Leaf1 base_lab_Leaf2: - base_lab_Leaf2 #base_lab_Leaf1.sls proxy: proxytype: napalm driver: eos host: base_lab_Leaf1 username: arista password: arista pyeapi: username: arista password: arista transport: http The top file will command the proxy to pickup the pillars for base_lab_Leaf1.sls which allows the execution modules of both proxy modules for pyeapi and napalm. So the execution modules for either will work ie *salt 'base_lab_Leaf1.sls' net.arp' will display the arp packets. Since napalm is the same as the net execution module.","title":"Proxy minions and agents"},{"location":"minions/#salt-discovery","text":"Upon a salt minion being installed it needs to find the salt master. The default way is having a dns lookup for the hostname of salt this is also configurable within the minion config file under /etc/salt/minion #/etc/salt/minion master: 1.2.3.4 The salt minion will then try to negotiate a secure connection to 1.2.3.4 once the connection is made the salt master will have to accept the connection. This can either be manual or automatic. The good part about salt versus other configuration management platforms is that salt does not require a username/password. In this example the salt minion is brought online. Once online it has to find its master. In a manual mode the master issues a 'salt-key -L' which will scan for new nodes. The 'salt-key -A' command will accept all new minion nodes.","title":"Salt discovery"},{"location":"minions/#salt-native-minion","text":"A salt native minion is the typical way of installing a linux minion that will sit on the device and communicate with the underlying host itself and send the data back to the master. At the time of writing this the native minion is almost GA and will be out at any point. Right now the minion will use either pyeapi or napalm to configure the device.","title":"Salt native minion"},{"location":"minions/#salt-native-minion_1","text":"Running the lab we typically start with salt-proxy --proxyid=base_lab_Leaf1 -d what this does is run a proxy process using the proxyid of base_lab_leaf1 salt-proxy itself will read the /srv/salt/pillars/top.sls file and find the proxies in which it needs to match base_lab_leaf1 for the proxy info. #/srv/salt/pillar/top.sls base: base_lab_Spine1: - base_lab_Spine1 base_lab_Spine2: - base_lab_Spine2 base_lab_Leaf1: - base_lab_Leaf1 base_lab_Leaf2: - base_lab_Leaf2 #base_lab_Leaf1.sls proxy: proxytype: napalm driver: eos host: base_lab_Leaf1 username: arista password: arista pyeapi: username: arista password: arista transport: http The top file will command the proxy to pickup the pillars for base_lab_Leaf1.sls which allows the execution modules of both proxy modules for pyeapi and napalm. So the execution modules for either will work ie *salt 'base_lab_Leaf1.sls' net.arp' will display the arp packets. Since napalm is the same as the net execution module.","title":"Salt native minion"},{"location":"pillars/","text":"Pillars are secrets/sensitive data which are stored as variables. For example, to start the salt proxy minion the salt proxy minion has to know the switch it needs to proxy to as well as a username/password. Running the lab we typically start with salt-proxy --proxyid=base_lab_Leaf1 -d what this does is run a proxy process using the proxyid of base_lab_leaf1 salt-proxy itself will read the /srv/salt/pillars/top.sls file and find the proxies in which it needs to match base_lab_leaf1 for the proxy info. These are all pillars files. #/srv/salt/pillar/top.sls base: base_lab_Spine1: - base_lab_Spine1 base_lab_Spine2: - base_lab_Spine2 base_lab_Leaf1: - base_lab_Leaf1 base_lab_Leaf2: - base_lab_Leaf2 #base_lab_Leaf1.sls proxy: proxytype: napalm driver: eos host: base_lab_Leaf1 username: arista password: arista pyeapi: username: arista password: arista transport: http To find the given pillar items per node you would issue the following. salt 'base_lab_Leaf1' pillar.items base_lab_Leaf1: ---------- proxy: ---------- driver: eos host: base_lab_Leaf1 password: arista proxytype: napalm username: arista pyeapi: ---------- host: base_lab_Leaf1 password: arista transport: http username: arista","title":"Pillars"},{"location":"prerequisite/","text":"For this lab to work it is expected that the user has the following knowledge base. Some history with Arista EOS Some history with Linux Some history with networking as a general topic. Some docker knowledge. Technical requirements. Docker (This was tested on Docker CE 19.03.13) Arista cEOS lab. (4.23.2F) Please sign up for a arista login and use the software downloads page and instructions on how to import a cEOS lab docker image. Docker-topo Docker-topo allows a user to quickly spin up a cEOS lab environment that connects the docker networks together for the user in portable fashion allowing the user to add custom docker containers and networks that they chose. Make sure that the config directory which stores the device configs is rw from the host to the ceos lab node. I was brave and did a chomod -R 777 /config I am sure you do not have to open up the permissions to that level. However, if you do not you will receive issues that the startup-config cannot be modified from cEOS. If you are a mac user and use docker for mac you will need to share this directory to allow the host to mount to the container. The base_lab.yml contains the general ground work for docker-topo. Which can be found within the root of the repo. Starting the lab.","title":"prerequisite"},{"location":"prerequisite/#starting-the-lab","text":"","title":"Starting the lab."},{"location":"reactors/","text":"","title":"Reactors"},{"location":"salt-general/","text":"The purpose of this repo is to demonstrate the power of salt with arista eos to orchestrate, automate and react to events within the network. Salt is a event drivien automation platform. This is different than the conventional network automation configuration platform tool as salt is able to not only generate configurations or automate certain software. Salt is also able to react to events with its ZeroMQ messaging system which sends constant events from the salt master to the salt minions. Salt was created in 2011 to stop a lot of manual work on servers. So a person would typically log into a linux server and apply application configuration to a device manually. Obviously, like network devices doing everything by hand is extremely time consuming. Salt started off as a configuration management only simply having a agent on each of the linux machines which then talked back to the master. Salt is written in 100% python which is extremely nice because a lot of salt can be modified to the users needs. Salt supports many different templating languages the most popular is Jinja. The general idea of salt is that everything is done within the master node(s). The master like the name suggests controls the minions. The master does remote execution which means it will send data to run through the minion to be remotely executed. In the state of the minion agent EOS opens up a unix socket which the device talks back to itself upon. Salt can be installed within a few minutes within this link . Common questions Salt vs Saltstack. Salt is the open source version of saltstack open for any developement. What ports need to be open for the salt master and salt minions to talk? UCP/TCP port 4505 and 4506. How is this differnt than Ansible, Chef and Puppet? Salt accomplishes the same thing as the other popular configuration management tools ie it will apply configuration, render templates etc. However, what is extremely unique to salt is that it has a modern ZeroMQ bus between minions and master node(s). So anything that happens within the salt minion is sent to the salt master as a sort of event. These events can even be sent externally to logging systems or salt can react to events that happen over the network bus. The idea is that instead of a human constantly creating playbooks or making changes the system operates autonomously. Salt like other configuration management systems has 3rd party integration into devices other than networking devices ie VMware, AWS etc. How does the minion talk to the master? With the salt agent the minion will talk to the salt master over the ZeroMQ bus over TCP ports 4506. The interesting part of the way salt works is that since it is a secure connection that the master has to accept there is no username/password that is required for the salt agent to talk to the master. This also means every time a device is brought online the master has to accept it. This can be automated but also allows for a secure connection instead of a constant api connection where a username and password are needed. What is the network driver within salt? One of the many advantages of saltstack is that it can communicate to the minions through the ZeroMQ bus in a datastructure in which it understands. In that datastructure the master simply passes data in the bus instructing the minion to use a module which is installed on the minion and pass data into it. The drivers that are commonly used are Napalm or net module and pyeapi","title":"Purpose of this repo"},{"location":"salt-general/#common-questions","text":"","title":"Common questions"},{"location":"salt-general/#salt-vs-saltstack","text":"Salt is the open source version of saltstack open for any developement.","title":"Salt vs Saltstack."},{"location":"salt-general/#what-ports-need-to-be-open-for-the-salt-master-and-salt-minions-to-talk","text":"UCP/TCP port 4505 and 4506.","title":"What ports need to be open for the salt master and salt minions to talk?"},{"location":"salt-general/#how-is-this-differnt-than-ansible-chef-and-puppet","text":"Salt accomplishes the same thing as the other popular configuration management tools ie it will apply configuration, render templates etc. However, what is extremely unique to salt is that it has a modern ZeroMQ bus between minions and master node(s). So anything that happens within the salt minion is sent to the salt master as a sort of event. These events can even be sent externally to logging systems or salt can react to events that happen over the network bus. The idea is that instead of a human constantly creating playbooks or making changes the system operates autonomously. Salt like other configuration management systems has 3rd party integration into devices other than networking devices ie VMware, AWS etc.","title":"How is this differnt than Ansible, Chef and Puppet?"},{"location":"salt-general/#how-does-the-minion-talk-to-the-master","text":"With the salt agent the minion will talk to the salt master over the ZeroMQ bus over TCP ports 4506. The interesting part of the way salt works is that since it is a secure connection that the master has to accept there is no username/password that is required for the salt agent to talk to the master. This also means every time a device is brought online the master has to accept it. This can be automated but also allows for a secure connection instead of a constant api connection where a username and password are needed.","title":"How does the minion talk to the master?"},{"location":"salt-general/#what-is-the-network-driver-within-salt","text":"One of the many advantages of saltstack is that it can communicate to the minions through the ZeroMQ bus in a datastructure in which it understands. In that datastructure the master simply passes data in the bus instructing the minion to use a module which is installed on the minion and pass data into it. The drivers that are commonly used are Napalm or net module and pyeapi","title":"What is the network driver within salt?"},{"location":"saltmines/","text":"","title":"Salt mines"},{"location":"sproxy/","text":"Salt proxies are used a proxy for devices within salt that cannot run an agent locally on the minion. For example, network switches that cannot run a minion or VMware ESXi that cannot run a linux process for the minion directly within the hypervisor. When it comes to proxies the proxy should not have to be ran at all times because the only time we really care about the proxy is when we are truly using salt events like rendering configuration or general salt events. That being said the salt super proxy was born(sproxy). The idea behind the sproxy is that it will not always run in a different location or on the master consuming memory. The sproxy will only run when the client is then targetting a salt aspect. Installing sproxy services pip3 install salt-sproxy check to see if the master is running service salt-master status If not start the salt-master with the following service salt-master start Accept 1 proxy minion salt-proxy --proxyid=base_lab_Leaf1 -d root@a00c23e5231c:/srv/salt# salt-key -A The following keys are going to be accepted: Unaccepted Keys: base_lab_Leaf1 Proceed? [n/Y] Y root@a00c23e5231c:/srv/salt# salt-key -L Accepted Keys: base_lab_Leaf1 Denied Keys: Unaccepted Keys: Rejected Keys: root@a00c23e5231c:/srv/salt# salt '*' test.ping base_lab_Leaf1: True Everything seems to work and we have a proxy running for base_lab_Leaf1. We do not have a proxy running for the other 3 devices. This is where sproxy comes into place. Because we have a pillar file setup for the other devices sproxy will read the pillar file and know that it typically is a proxy minion. Lets test this out with base_lab_Leaf2. Keep in mind the master DOES NOT know that this proxy minion exsits. root@a00c23e5231c:/srv/salt# salt-sproxy 'base_lab_Leaf2' test.ping base_lab_Leaf2: True root@a00c23e5231c:/srv/salt# salt-sproxy 'base_lab_Leaf2' net.cli 'show version' base_lab_Leaf2: ---------- comment: out: ---------- show version: cEOSLab Hardware version: Serial number: System MAC address: 0242.c083.e816 Software image version: 4.23.2F Architecture: i686 Internal build version: 4.23.2F-15405360.4232F Internal build ID: 4cde5c53-3642-4934-8bcc-05691ffd79b3 cEOS tools version: 1.1 Uptime: 2 weeks, 5 days, 17 hours and 13 minutes Total memory: 32970568 kB Free memory: 18791272 kB result: True So the sproxy will run every time the command is listed then it will turn down every time it is finished. Right now this will work with either a master config or with what is called a roster file. This makes sproxy a very attractive use case for things like one time rendering the way a typical ansible playbook would work.","title":"Sproxy"},{"location":"sproxy/#installing-sproxy-services","text":"pip3 install salt-sproxy check to see if the master is running service salt-master status If not start the salt-master with the following service salt-master start","title":"Installing sproxy services"},{"location":"sproxy/#accept-1-proxy-minion","text":"salt-proxy --proxyid=base_lab_Leaf1 -d root@a00c23e5231c:/srv/salt# salt-key -A The following keys are going to be accepted: Unaccepted Keys: base_lab_Leaf1 Proceed? [n/Y] Y root@a00c23e5231c:/srv/salt# salt-key -L Accepted Keys: base_lab_Leaf1 Denied Keys: Unaccepted Keys: Rejected Keys: root@a00c23e5231c:/srv/salt# salt '*' test.ping base_lab_Leaf1: True Everything seems to work and we have a proxy running for base_lab_Leaf1. We do not have a proxy running for the other 3 devices. This is where sproxy comes into place. Because we have a pillar file setup for the other devices sproxy will read the pillar file and know that it typically is a proxy minion. Lets test this out with base_lab_Leaf2. Keep in mind the master DOES NOT know that this proxy minion exsits. root@a00c23e5231c:/srv/salt# salt-sproxy 'base_lab_Leaf2' test.ping base_lab_Leaf2: True root@a00c23e5231c:/srv/salt# salt-sproxy 'base_lab_Leaf2' net.cli 'show version' base_lab_Leaf2: ---------- comment: out: ---------- show version: cEOSLab Hardware version: Serial number: System MAC address: 0242.c083.e816 Software image version: 4.23.2F Architecture: i686 Internal build version: 4.23.2F-15405360.4232F Internal build ID: 4cde5c53-3642-4934-8bcc-05691ffd79b3 cEOS tools version: 1.1 Uptime: 2 weeks, 5 days, 17 hours and 13 minutes Total memory: 32970568 kB Free memory: 18791272 kB result: True So the sproxy will run every time the command is listed then it will turn down every time it is finished. Right now this will work with either a master config or with what is called a roster file. This makes sproxy a very attractive use case for things like one time rendering the way a typical ansible playbook would work.","title":"Accept 1 proxy minion"},{"location":"states/","text":"States are exactly what they sound like. A state is the state of a given salt minion. For example, a state might be a one time configuration to configure a server for things like apache and mysql. In our lab there is a state to render configuration and a state to then push the configuration. States are a deterministic way to push a state to a salt minion. Lets look at one of the states for the basic bgp configuration lab. #/srv/salt/states/basic_bgp_render.sls render the output: file.managed: - name: /srv/salt/templates/intended/configs/{{ hostname }}.cfg - source: /srv/salt/templates/basic_bgp.j2 - template: jinja Taking this state line by line. render the output: - This can be anything just the name of the state. file.managed: - Using the file module to later pass in name, source and template. name: /srv/salt/templates/intended/configs/{{ hostname }}.cfg - This is telling the file module to save any rendered config to that path name. So for example if we run the state salt 'leaf1' state.sls basic_bgp_render this will then output the file to /srv/salt/templates/intended/configs/leaf1 - source: /srv/salt/templates/basic_bgp.j2 - This is the source of the bgp template we are rendering. - template: jinja - use the jinja2 templating engine. Running the sale state To run any salt state the state.sls execution module has to be ran. Note do not include the .sls extension when running basic_bgp_render also, if you put a file directory called bgp within the states file you must call bgp.basic_bgp_render as salt uses python dotted notation. salt 'base_lab_Leaf1' state.sls basic_bgp_render base_lab_Leaf1: ---------- ID: render the output Function: file.managed Name: /srv/salt/templates/intended/configs/base_lab_Leaf1.cfg Result: True Comment: File /srv/salt/templates/intended/configs/base_lab_Leaf1.cfg is in the correct state Started: 14:59:30.537848 Duration: 361.574 ms Changes: Summary for base_lab_Leaf1 ------------ Succeeded: 1 Failed: 0 ------------ Total states run: 1 Total run time: 361.574 ms","title":"States"},{"location":"states/#running-the-sale-state","text":"To run any salt state the state.sls execution module has to be ran. Note do not include the .sls extension when running basic_bgp_render also, if you put a file directory called bgp within the states file you must call bgp.basic_bgp_render as salt uses python dotted notation. salt 'base_lab_Leaf1' state.sls basic_bgp_render base_lab_Leaf1: ---------- ID: render the output Function: file.managed Name: /srv/salt/templates/intended/configs/base_lab_Leaf1.cfg Result: True Comment: File /srv/salt/templates/intended/configs/base_lab_Leaf1.cfg is in the correct state Started: 14:59:30.537848 Duration: 361.574 ms Changes: Summary for base_lab_Leaf1 ------------ Succeeded: 1 Failed: 0 ------------ Total states run: 1 Total run time: 361.574 ms","title":"Running the sale state"}]}